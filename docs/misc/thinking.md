
# Thinking

AI模型可以理解为一个复杂的函数`f(x)`，模型的训练和微调目的是为了确保输入可以得到最优的解。<br>
模型的运行与传统数学软件、计算机程序有所不同，对于计算资源和架构有一定的要求，GPU等支持并行计算的系统比较适合。

因此模型一般涉及如下几个方面：

1. 模型的架构设计
2. 数据集准备
3. 模型训练
4. 模型微调
5. 模型量化

GPT是一个语言模型，更擅长自然语言处理，不适合做真正的深度计算（比如，物理学中的某种模型计算）。

GPT由多层神经网络构成，通过`word embedding`将词语转换成高维的向量坐标，这样只能处理数字的神经网络层也可以处理人类的语言。

每一层对于这些向量进行加权求和再传递给下一层，直至最后输出。

比如手写数字的识别，0-9这10个数字，每个数字图像转换成1维的灰度向量。将其输入给模型，模型的输出为10个数字，每个数字代表结果分别是0-9的概率。

经过一定的后处理，我们就可以获得一个基于模型的手写数字识别软件。

GPT等模型采用前馈网络，将输入单向地在多层神经网络中向前传递计算，直至最终的输出，再重新“审视”当前的整个句子调整输出，以此迭代下去，直至结束。<br>
这与人脑的处理是有相似和差异的，人脑也会激活相关的神经元，但是它以我们不可名状的方式综合处理得到结果，而不是GPT这样机械地重复上述前馈网络机制的这个过程。

通过Google的Attention Is All You Need这篇论文，OpenAI加入了注意力机制。在处理我们输入的句子中，它也会借鉴人类的处理机制，提取句子中的重点词语，从而能够处理长文本。<br>
通过强化学习的方案，开发人员训练出一个奖励模型来监督调教GPT的模型训练。

## AI将编程的表达能力向问题域迈进一大步

在编译原理的书里提到，编译器的作用是用来填补问题域和实现域的鸿沟：我们使用编程语言来描述问题的解决方案，编译器将我们的代码来转换成可以在计算机中运行的特定格式文件。
如果一门语言的表达能力越接近实现域（比如，C语言），越需要程序员来进行大量的抽象设计，程序员的工作量越大。
当编程语言的表达能力越抽象，越接近实际问题时（比如，Python），背后的编译器/解释器承担的工作越多。

而如今，LLM使得自然语言可以成为描述问题解决方案的“编程语言”，大模型的工作流是背后的“编译器”。
